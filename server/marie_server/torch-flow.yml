jtype: Flow
version: '1'
shared_config:
  storage: &storage_conf
    storage_enabled: True
    storage_conf:
      provider: postgresql
      hostname: 127.0.0.1
      port: 5432
      username: postgres
      password: 123456
      database: postgres
      default_table: shared_docs

  message: &message_conf
    messaging_enabled : True
    messaging_conf :
      provider: amazon-rabbitmq
      hostname: ${{ ENV.messaging_hostname }}
      port: 5671
      username: ${{ ENV.messaging_username }}
      password: ${{ ENV.messaging_password }}
      tls: True
      virtualhost: /marie-ai

with:
  port:
    - 51000
    - 52000
  protocol:
    - http
    - grpc
  discovery: True
  discovery_host: 127.0.0.1
  discovery_port: 8500

  event_tracking: True

  expose_endpoints:
    /extract:
      methods: ["POST"]
      summary: Extract data-POC
      tags:
        - extract
    /status:
      methods: ["POST"]
      summary: Status
      tags:
        - extract

    /text/status:
      methods: ["POST"]
      summary: Extract data
      tags:
        - extract

    /ner/extract:
      methods: ["POST"]
      summary: Extract NER
      tags:
        - ner



executors:

#  - name: extract_t
#    uses:
#      jtype: TextExtractionExecutor
##      jtype: ExtractExecutor
#      with:
#        messaging_enabled : True
#        messaging_conf :
#          provider: amazon-rabbitmq
#          hostname: b-ff3d0999-a25b-4a5a-9775-e7ba76f8fa3d.mq.us-east-1.amazonaws.com
#          port: 5671
#          username: A
#          password: V
#          tls: True
#          virtualhost: /marie-ai
#      metas:
#        py_modules:
##          - marie_server.executors.extract.mserve_torch
#          - marie.executor.text
#    timeout_ready: 3000000
#    replicas: ${{ CONTEXT.gpu_device_count }}
#    env :
#      CUDA_VISIBLE_DEVICES: RR

#  - name: ner_t
#    uses:
#      jtype: NerExtractionExecutor
#      with:
#        model_name_or_path : 'rms/layoutlmv3-large-corr-ner'
#        <<: *storage_conf
#        storage_enabled : False
#      metas:
#        py_modules:
##          - marie_server.executors.ner.mserve_torch
#          - marie.executor.ner
#    timeout_ready: 3000000
##    replicas: 1
#    replicas: ${{ CONTEXT.gpu_device_count }}
#    env :
#      CUDA_VISIBLE_DEVICES: RR

  - name: overlay_t
    uses:
      jtype: OverlayExecutor
      with:
        model_name_or_path : 'rms/holder'
        <<: *storage_conf
        storage_enabled : True
      metas:
        py_modules:
          - marie.executor.overlay
    timeout_ready: 3000000
    replicas: 1

